"""
Data combiner for joining decoder files with main data.
Reads YAML configuration and performs all specified joins.

Uses the storage abstraction layer to support disk, MinIO, and PostgreSQL backends.
Set STORAGE_BACKEND environment variable to switch backends.

Functions:
    [M] load_yaml_config() - Load and parse YAML configuration
    [M] load_main_data() - Load processed main data from 02-processed
    [M] load_decoder_data() - Load and process decoder files
    [M] check_unique_join_columns() - Verify join columns are unique
    [M] generate_column_prefix() - Generate prefixes based on case style
    [M] perform_simple_join() - Execute simple column joins
    [M] perform_complex_join() - Execute multi-column joins
    [M] combine_all_data() - Main orchestrator function
    [M] main() - Command line entry point
"""

import sys
import yaml
import polars as pl
import datetime
import argparse
from pathlib import Path
from rich.console import Console
from rich.progress import Progress, SpinnerColumn, BarColumn, TextColumn, TimeElapsedColumn
from io import StringIO

# Add project root to Python path for imports
project_root = Path(__file__).resolve().parents[3]  # Go up 3 levels: core -> backend -> src -> project_root
sys.path.insert(0, str(project_root))

from src.backend.core.converter import convert_case
from backend.io import storage_context, get_config

console = Console()

class CombinerLogger:
    """
    Logger class for capturing combiner output to both console and log files.
    Uses storage abstraction layer for file operations.
    """
    def __init__(self, output_dir: str | None = None):
        config = get_config()
        self.output_dir = output_dir or config.paths.combined_dir
        self.log_dir = f"{self.output_dir}/logs"

        # Create timestamp
        self.timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")

        # Setup log file paths (relative to storage base)
        self.timestamped_log_file = f"{self.log_dir}/combiner_log_{self.timestamp}.txt"
        self.latest_log_file = f"{self.log_dir}/combiner_log_latest.txt"

        # Initialize log content
        self.log_content = StringIO()

        # Setup console with file capture
        self.console = Console(file=self.log_content, width=120)

        # Create log directory via storage backend
        with storage_context() as storage:
            storage.makedirs(self.log_dir)

    def print(self, *args, **kwargs):
        """Print to both console and log file"""
        # Print to actual console
        console.print(*args, **kwargs)
        # Print to captured log
        self.console.print(*args, **kwargs)

    def save_logs(self):
        """Save captured logs to files via storage backend"""
        log_text = self.log_content.getvalue()
        log_bytes = log_text.encode('utf-8')

        with storage_context() as storage:
            # Write timestamped log
            storage.write_bytes(log_bytes, self.timestamped_log_file)
            # Write latest log
            storage.write_bytes(log_bytes, self.latest_log_file)

        console.print(f"[green]Logs saved to: {self.timestamped_log_file}")
        console.print(f"[green]Latest log: {self.latest_log_file}")

    def get_log_files(self):
        """Return paths to the log files"""
        return {
            "timestamped_log": self.timestamped_log_file,
            "latest_log": self.latest_log_file,
            "timestamp": self.timestamp
        }

def load_yaml_config(config_path: str | None = None, logger=None):
    """
    Load and parse YAML configuration file via storage backend.

    Args:
        config_path: Path to YAML config. Defaults to STORAGE_METADATA_DIR/decoder_mapping_config.yaml
    """
    if logger is None:
        logger = console

    config_settings = get_config()
    if config_path is None:
        config_path = f"{config_settings.paths.metadata_dir}/decoder_mapping_config.yaml"

    try:
        with storage_context() as storage:
            yaml_bytes = storage.read_bytes(config_path)
            config = yaml.safe_load(yaml_bytes.decode('utf-8'))
        logger.print(f"[green]YAML configuration loaded: {len(config['decoder_mappings'])} mappings found")
        return config
    except Exception as e:
        logger.print(f"[red]Error loading YAML config: {str(e)}")
        raise

def load_main_data(processed_dir: str | None = None, logger=None):
    """
    Load main data files from processed directory via storage backend.
    Returns dict of dataframes keyed by filename.

    Args:
        processed_dir: Directory path. Defaults to STORAGE_PROCESSED_DIR env var.
    """
    if logger is None:
        logger = console

    config = get_config()
    processed_dir = processed_dir or config.paths.processed_dir

    main_data = {}

    with storage_context() as storage:
        # Check if directory exists
        if not storage.exists(processed_dir):
            logger.print(f"[red]Processed directory not found: {processed_dir}")
            return main_data

        # Find CSV files (converted main data) - exclude decoder files
        csv_files = storage.list_files(processed_dir, "*.csv")
        csv_files = [f for f in csv_files
                     if not Path(f).name.endswith('_encrypted.csv')
                     and not Path(f).name.startswith('Dec_')]

        logger.print(f"[cyan]Loading main data files from {processed_dir}")

        for csv_file in csv_files:
            try:
                filename = Path(csv_file).name
                filestem = Path(csv_file).stem

                # Load data via storage backend
                df = storage.read_dataframe(csv_file, format="csv")
                logger.print(f"[green]  Loaded: {filename} ({len(df.columns)} columns)")

                main_data[filestem] = df

            except Exception as e:
                logger.print(f"[red]  Error loading {csv_file}: {str(e)}")

    return main_data

def detect_csv_separator(storage, file_path: str) -> str:
    """
    Automatically detect CSV separator by checking the first line.
    Uses storage backend to read the file.
    """
    # Read first chunk of file to get the first line
    file_bytes = storage.read_bytes(file_path)
    # Decode and get first line
    first_line = file_bytes.decode('latin1').split('\n')[0].strip()

    # Count occurrences of potential separators
    separators = [';', ',', '|']
    separator_counts = {sep: first_line.count(sep) for sep in separators}

    # Return the separator with the highest count (most likely to be the delimiter)
    best_separator = max(separator_counts, key=separator_counts.get)
    return best_separator if separator_counts[best_separator] > 0 else ','

def load_decoder_data(decoder_file: str, input_dir: str | None = None, logger=None):
    """
    Load and process a decoder file with automatic separator detection.
    Uses storage backend for file operations.

    Args:
        decoder_file: Name of decoder file
        input_dir: Directory path. Defaults to STORAGE_PROCESSED_DIR env var.
    """
    if logger is None:
        logger = console

    config = get_config()
    input_dir = input_dir or config.paths.processed_dir
    decoder_path = f"{input_dir}/{decoder_file}"

    with storage_context() as storage:
        if not storage.exists(decoder_path):
            logger.print(f"[red]Decoder file not found: {decoder_file}")
            return None

        try:
            # Auto-detect separator
            separator = detect_csv_separator(storage, decoder_path)
            logger.print(f"[cyan]Detected separator '{separator}' for {decoder_file}")

            # Read file bytes for processing
            file_bytes = storage.read_bytes(decoder_path)
            file_content = file_bytes.decode('latin1')

            # Special handling for Dec_vooropl.csv - force first column as string
            if 'vooropl' in decoder_file.lower():
                # Get column names from header
                header_line = file_content.split('\n')[0].strip()
                first_col = header_line.split(separator)[0]

                # Create schema with first column as string, others auto-detect
                schema_overrides = {first_col: pl.String}

                logger.print(f"[yellow]Forcing first column '{first_col}' to string type for {decoder_file}")

                # Use StringIO for Polars to read from string
                from io import StringIO
                df = pl.read_csv(StringIO(file_content),
                               separator=separator,
                               schema_overrides=schema_overrides,
                               try_parse_dates=False)
            else:
                # Standard loading for other files
                from io import StringIO
                df = pl.read_csv(StringIO(file_content),
                               separator=separator,
                               try_parse_dates=False,
                               infer_schema_length=None)

            logger.print(f"[green]  Loaded decoder: {decoder_file} ({len(df.columns)} columns, {len(df)} rows)")
            return df
        except Exception as e:
            logger.print(f"[red]Error loading decoder file {decoder_file}: {str(e)}")
            return None

def load_mapping_tables_config(config_path: str | None = None, logger=None):
    """
    Load and parse mapping tables configuration file via storage backend.

    Args:
        config_path: Path to config. Defaults to STORAGE_METADATA_DIR/mapping_tables_config.yaml
    """
    if logger is None:
        logger = console

    config_settings = get_config()
    if config_path is None:
        config_path = f"{config_settings.paths.metadata_dir}/mapping_tables_config.yaml"

    try:
        with storage_context() as storage:
            if not storage.exists(config_path):
                logger.print(f"[yellow]Mapping tables config not found: {config_path}, loading all files")
                return None, None

            yaml_bytes = storage.read_bytes(config_path)
            config = yaml.safe_load(yaml_bytes.decode('utf-8'))

        # Extract active (non-commented) files from the mapping_tables list
        active_files = []
        if 'mapping_tables' in config:
            active_files = [f for f in config['mapping_tables'] if f is not None]

        logger.print(f"[green]Mapping tables configuration loaded: {len(active_files)} active files")
        return config, active_files
    except Exception as e:
        logger.print(f"[red]Error loading mapping tables config: {str(e)}")
        return None, None

def load_reference_data(reference_dir: str | None = None, active_files=None, logger=None):
    """
    Load reference files from reference directory via storage backend.
    If active_files is provided, only load those files that are active in the config.
    Returns dict of dataframes keyed by filename (without .csv extension).

    Args:
        reference_dir: Directory path. Defaults to STORAGE_REFERENCE_DIR env var.
    """
    if logger is None:
        logger = console

    config = get_config()
    reference_dir = reference_dir or config.paths.reference_dir

    reference_data = {}

    with storage_context() as storage:
        if not storage.exists(reference_dir):
            logger.print(f"[yellow]Reference directory not found: {reference_dir}")
            return reference_data, [], []

        # Find all CSV files in reference directory
        all_csv_files = storage.list_files(reference_dir, "*.csv")
        all_filenames = [Path(f).name for f in all_csv_files]

        # If active_files is provided, check which ones exist and which are missing
        if active_files is not None:
            # Check which active files exist
            existing_active_files = []
            missing_active_files = []

            for active_file in active_files:
                if active_file in all_filenames:
                    existing_active_files.append(active_file)
                else:
                    missing_active_files.append(active_file)

            # Check which existing files are not in the config
            files_not_in_config = [f for f in all_filenames if f not in active_files]

            logger.print(f"[cyan]Loading reference files from {reference_dir}")
            logger.print(f"[cyan]Found {len(all_csv_files)} total files, {len(existing_active_files)} active in config")

            if missing_active_files:
                logger.print(f"[yellow]Missing active files: {missing_active_files}")

            if files_not_in_config:
                logger.print(f"[blue]Files not in config (inactive): {files_not_in_config}")

            # Only process existing active files
            files_to_process = [f for f in all_csv_files if Path(f).name in existing_active_files]
        else:
            # Load all files if no config provided
            files_to_process = all_csv_files
            existing_active_files = all_filenames
            missing_active_files = []
            files_not_in_config = []

            logger.print(f"[cyan]Loading all reference files from {reference_dir}")
            logger.print(f"[cyan]Found {len(all_csv_files)} reference files")

        # Load the files
        for csv_file in files_to_process:
            try:
                filename = Path(csv_file).name
                filestem = Path(csv_file).stem

                # Load the reference file via storage backend
                df = storage.read_dataframe(csv_file, format="csv")

                # Validate structure (should have 'value' and 'label' columns)
                if 'value' not in df.columns or 'label' not in df.columns:
                    logger.print(f"[yellow]Skipping {filename}: missing 'value' or 'label' columns")
                    continue

                # Store with filename as key (without .csv extension)
                reference_data[filestem] = df
                logger.print(f"[green]  Loaded reference: {filename} ({len(df)} rows)")

            except Exception as e:
                logger.print(f"[red]Error loading reference file {csv_file}: {str(e)}")

    return reference_data, missing_active_files, files_not_in_config

def find_matching_reference_column(column_name, reference_data, case_style="snake_case", logger=None):
    """
    Find a matching reference file for a given column name
    Returns (reference_key, reference_df) if found, (None, None) if not found
    """
    if logger is None:
        logger = console


    # Clean column name and convert to snake_case for matching
    clean_column = column_name.replace('_', ' ').strip()
    snake_case_column = convert_case(clean_column, 'snake_case')

    # Look for reference file with _label suffix
    reference_key = f"{snake_case_column}_label"
    if reference_key in reference_data:
        return reference_key, reference_data[reference_key]

    return None, None

def perform_reference_joins(df, reference_data, case_style="snake_case", logger=None):
    """
    Join reference data to add label columns for all matching columns
    Returns (updated_df, successful_joins, failed_joins, non_working_files)
    """
    if logger is None:
        logger = console


    result_df = df.clone()
    successful_joins = 0
    failed_joins = 0
    non_working_files = []

    logger.print(f"[cyan]ğŸ”— Adding reference labels to columns...")

    # Process each column in the dataframe
    for column in df.columns:
        # Find matching reference file
        ref_key, ref_df = find_matching_reference_column(column, reference_data, case_style, logger)

        if ref_df is not None:
            try:
                # Generate label column name with proper case style
                label_column_base = f"{column}_label"
                label_column = convert_case(label_column_base, case_style) if case_style != 'original' else label_column_base

                # Skip if label column already exists
                if label_column in result_df.columns:
                    continue

                # Prepare reference data for join
                ref_for_join = ref_df.select(['value', 'label']).rename({'label': label_column})

                # Check for datatype compatibility
                main_dtype = result_df[column].dtype
                ref_dtype = ref_for_join['value'].dtype

                if main_dtype != ref_dtype:
                    # Try to cast reference 'value' column to match main data type
                    try:
                        ref_for_join = ref_for_join.with_columns(
                            pl.col('value').cast(main_dtype)
                        )
                    except Exception:
                        # Fall back to string casting for both
                        ref_for_join = ref_for_join.with_columns(
                            pl.col('value').cast(pl.String)
                        )
                        # Join will cast main column to string on-the-fly

                # Perform left join
                try:
                    result_df = result_df.join(
                        ref_for_join,
                        left_on=column,
                        right_on='value',
                        how='left'
                    )
                    successful_joins += 1
                    logger.print(f"[green]  âœ… Added {label_column} from {ref_key}.csv")

                except Exception as join_error:
                    # Try string fallback
                    try:
                        result_df = result_df.with_columns(
                            pl.col(column).cast(pl.String).alias(f"{column}_str_temp")
                        ).join(
                            ref_for_join,
                            left_on=f"{column}_str_temp",
                            right_on='value',
                            how='left'
                        ).drop(f"{column}_str_temp")
                        successful_joins += 1
                        logger.print(f"[green]  âœ… Added {label_column} from {ref_key}.csv (string fallback)")
                    except Exception as fallback_error:
                        logger.print(f"[red]âŒ Failed to join {ref_key}.csv to {column}: {str(fallback_error)}")
                        failed_joins += 1
                        non_working_files.append({
                            'file': f"{ref_key}.csv",
                            'column': column,
                            'error': str(fallback_error)
                        })

            except Exception as e:
                logger.print(f"[red]âŒ Error processing reference {ref_key}.csv for {column}: {str(e)}")
                failed_joins += 1
                non_working_files.append({
                    'file': f"{ref_key}.csv",
                    'column': column,
                    'error': str(e)
                })

    return result_df, successful_joins, failed_joins, non_working_files

def load_manual_mapping_config(config_path: str | None = None, logger=None):
    """
    Load and parse manual mapping tables configuration file via storage backend.

    Args:
        config_path: Path to config. Defaults to STORAGE_METADATA_DIR/manual_tables_config.yaml
    """
    if logger is None:
        logger = console

    config_settings = get_config()
    if config_path is None:
        config_path = f"{config_settings.paths.metadata_dir}/manual_tables_config.yaml"

    try:
        with storage_context() as storage:
            if not storage.exists(config_path):
                logger.print(f"[yellow]Manual mapping config not found: {config_path}")
                return None, None

            yaml_bytes = storage.read_bytes(config_path)
            config = yaml.safe_load(yaml_bytes.decode('utf-8'))

        # Extract active mappings from the manual_mapping_tables list
        active_mappings = []
        if 'manual_mapping_tables' in config:
            for mapping in config['manual_mapping_tables']:
                if mapping is not None:
                    # Handle both old format (just filenames) and new format (dict with file, join_on, output_column)
                    if isinstance(mapping, str):
                        # Old format - just filename
                        active_mappings.append({
                            'file': mapping,
                            'join_on': None,  # Will be auto-detected
                            'output_column': None  # Will be auto-generated
                        })
                    elif isinstance(mapping, dict):
                        # New format - dict with configuration
                        active_mappings.append(mapping)

        logger.print(f"[green]Manual mapping config loaded: {len(active_mappings)} manual mapping configurations")
        return config, active_mappings
    except Exception as e:
        logger.print(f"[red]Error loading manual mapping config: {str(e)}")
        return None, None

def load_manual_mapping_data(manual_dir: str | None = None, active_mappings=None, logger=None):
    """
    Load manual mapping files from manual directory via storage backend.
    Returns dict of dataframes keyed by filename (without .csv extension), and mapping configurations.

    Args:
        manual_dir: Directory path. Defaults to STORAGE_REFERENCE_DIR/manual.
    """
    if logger is None:
        logger = console

    config = get_config()
    manual_dir = manual_dir or f"{config.paths.reference_dir}/manual"

    manual_data = {}
    mapping_configs = {}

    with storage_context() as storage:
        if not storage.exists(manual_dir):
            logger.print(f"[yellow]Manual mapping directory not found: {manual_dir}")
            return manual_data, mapping_configs, []

        # Find all CSV files in manual directory
        all_csv_files = storage.list_files(manual_dir, "*.csv")

        if not all_csv_files:
            logger.print(f"[yellow]No CSV files found in manual directory: {manual_dir}")
            return manual_data, mapping_configs, []

        # Filter files based on active_mappings configurations
        files_to_load = []
        missing_active_files = []

        if active_mappings:
            for mapping_config in active_mappings:
                filename = mapping_config['file']
                file_path = f"{manual_dir}/{filename}"

                if storage.exists(file_path):
                    files_to_load.append((file_path, mapping_config))
                else:
                    missing_active_files.append(filename)
        else:
            # Load all files if no config provided
            for csv_file in all_csv_files:
                filename = Path(csv_file).name
                # Create default mapping config
                default_config = {
                    'file': filename,
                    'join_on': None,  # Will be auto-detected
                    'output_column': None  # Will be auto-generated
                }
                files_to_load.append((csv_file, default_config))

        logger.print(f"[cyan]Loading {len(files_to_load)} manual mapping files...")

        for csv_file, mapping_config in files_to_load:
            try:
                filename = Path(csv_file).name
                filestem = Path(csv_file).stem

                # Read the manual mapping file via storage backend
                df = storage.read_dataframe(csv_file, format="csv")

                # Validate that it has the expected columns
                if not all(col in df.columns for col in ['value', 'label']):
                    logger.print(f"[yellow]Manual mapping file {filename} missing required columns (value, label)")
                    continue

                # Store with filename as key (without .csv extension)
                manual_data[filestem] = df
                mapping_configs[filestem] = mapping_config
                logger.print(f"[green]  Loaded manual mapping: {filename} ({len(df)} mappings)")

            except Exception as e:
                logger.print(f"[red]Error loading manual mapping file {csv_file}: {str(e)}")

        if missing_active_files:
            logger.print(f"[yellow]Manual mapping files not found: {missing_active_files}")

    return manual_data, mapping_configs, missing_active_files

def apply_manual_mappings(df, manual_data, mapping_configs, logger=None):
    """
    Apply manual categorical mappings to dataframe columns
    Uses configuration to determine join columns and output column names
    """
    if logger is None:
        logger = console

    result_df = df.clone()
    successful_mappings = 0
    failed_mappings = 0


    # Process each manual mapping configuration
    for key, manual_df in manual_data.items():
        if key not in mapping_configs:
            logger.print(f"[yellow]âš ï¸  No configuration found for {key}, skipping")
            continue

        config = mapping_configs[key]
        join_on = config.get('join_on')
        output_column = config.get('output_column')

        # Auto-detect join column if not specified
        if not join_on:
            # Try to find a matching column in the dataframe
            # Use the key (filename without .csv) as the starting point
            potential_columns = [
                key,
                convert_case(key.replace('_cat', '').replace('_NL', '').replace('_profiel_cat', '').replace('_per_jaar', '').replace('_per_5_jaar_cat', ''), 'snake_case')
            ]

            join_on = None
            for potential_col in potential_columns:
                if potential_col in df.columns:
                    join_on = potential_col
                    break

            if not join_on:
                logger.print(f"[yellow]âš ï¸  Could not find matching column for {key}, skipping")
                continue

        # Auto-generate output column name if not specified
        if not output_column:
            output_column = key  # Use filename without .csv as default

        # Check if join column exists
        if join_on not in df.columns:
            logger.print(f"[yellow]âš ï¸  Join column '{join_on}' not found for {key}, skipping")
            continue

        try:
            # Perform the mapping join
            result_df = result_df.join(
                manual_df.select([
                    pl.col('value'),
                    pl.col('label').alias(output_column)
                ]),
                left_on=join_on,
                right_on='value',
                how='left'
            )

            successful_mappings += 1
            logger.print(f"[green]  âœ… Added manual mapping: {join_on} â†’ {output_column}")

        except Exception as e:
            # Try string fallback
            try:
                result_df = result_df.with_columns(
                    pl.col(join_on).cast(pl.String).alias(f"{join_on}_str_temp")
                ).join(
                    manual_df.select([
                        pl.col('value').cast(pl.String),
                        pl.col('label').alias(output_column)
                    ]),
                    left_on=f"{join_on}_str_temp",
                    right_on='value',
                    how='left'
                ).drop(f"{join_on}_str_temp")

                successful_mappings += 1
                logger.print(f"[green]  âœ… Added manual mapping: {join_on} â†’ {output_column} (string fallback)")

            except Exception as fallback_error:
                logger.print(f"[red]âŒ Failed manual mapping {key}: {str(fallback_error)}")
                failed_mappings += 1

    return result_df, successful_mappings, failed_mappings

def check_unique_join_columns(df, join_columns):
    """
    Check if join columns form unique combinations
    """
    if isinstance(join_columns, str):
        join_columns = [join_columns]

    # Check if all join columns exist
    missing_cols = [col for col in join_columns if col not in df.columns]
    if missing_cols:
        return False, f"Missing columns: {missing_cols}"

    # Check uniqueness
    unique_count = df.select(join_columns).unique().height
    total_count = df.height

    if unique_count != total_count:
        return False, f"Join columns not unique: {unique_count} unique vs {total_count} total rows"

    return True, "OK"

def generate_column_prefix(join_column, target_column, case_style="snake_case"):
    """
    Generate column prefix based on join column, target column and case style

    Formats:
    - snake_case: {join_column}_{target_column}
    - camelCase: {join_column}{TargetColumn}
    - PascalCase: {JoinColumn}{TargetColumn}
    - original: {join_column} {target_column}
    """

    # Clean up column names (remove special characters)
    join_clean = join_column.replace('(', '').replace(')', '').replace('-', ' ').strip()
    target_clean = target_column.replace('(', '').replace(')', '').replace('-', ' ').strip()

    if case_style == "snake_case":
        # Convert to snake_case and join with underscore
        join_part = convert_case(join_clean, "snake_case")
        target_part = convert_case(target_clean, "snake_case")
        return f"{join_part}_{target_part}"

    elif case_style == "camelCase":
        # First part lowercase, second part title case, no separator
        join_part = convert_case(join_clean, "camelCase")
        target_part = convert_case(target_clean, "PascalCase")  # PascalCase for second part
        return f"{join_part}{target_part}"

    elif case_style == "PascalCase":
        # Both parts title case, no separator
        join_part = convert_case(join_clean, "PascalCase")
        target_part = convert_case(target_clean, "PascalCase")
        return f"{join_part}{target_part}"

    else:  # "original"
        # Keep original case and join with space
        return f"{join_clean} {target_clean}"

def perform_simple_join(main_df, decoder_df, mapping_config, case_style="snake_case", logger=None):
    """
    Perform a simple join operation
    """
    if logger is None:
        logger = console

    main_column = mapping_config['main_column']
    join_column = mapping_config['join_column']
    additional_columns = mapping_config['additional_columns']

    # Apply case conversion to column names for matching
    main_column_converted = convert_case(main_column, case_style)
    join_column_converted = convert_case(join_column, case_style)
    additional_columns_converted = [convert_case(col, case_style) for col in additional_columns]

    # Check if main column exists
    if main_column_converted not in main_df.columns:
        logger.print(f"[yellow]âš ï¸  Main column not found: {main_column_converted}")
        return main_df

    # Dynamic column matching - try different case conversions if exact match fails
    if join_column_converted not in decoder_df.columns:
        logger.print(f"[yellow]âš ï¸  Exact join column not found: {join_column_converted}")
        logger.print(f"[cyan]ğŸ” Trying dynamic column matching...")

        # Try different case styles to find matching column
        for try_case in ['snake_case', 'camelCase', 'PascalCase', 'original']:
            if try_case == case_style:
                continue  # Skip the one we already tried

            try_join_column = convert_case(join_column, try_case)
            if try_join_column in decoder_df.columns:
                logger.print(f"[green]âœ… Found matching column with {try_case}: {try_join_column}")
                join_column_converted = try_join_column
                break
        else:
            # No match found with any case style
            logger.print(f"[red]âŒ No matching column found in any case style")
            logger.print(f"[cyan]Available columns: {decoder_df.columns}")
            return main_df

    # Check uniqueness of join column in decoder
    is_unique, msg = check_unique_join_columns(decoder_df, join_column_converted)
    if not is_unique:
        logger.print(f"[red]âŒ {msg}")
        return main_df

    # Prepare decoder columns with prefixes
    decoder_select_cols = [join_column_converted]
    rename_mapping = {}

    for add_col in additional_columns_converted:
        if add_col in decoder_df.columns:
            # Generate prefix using MAIN COLUMN (not join column) to ensure uniqueness
            prefix = generate_column_prefix(main_column_converted, add_col.replace('_', ' '), case_style)
            new_name = f"{prefix}"
            decoder_select_cols.append(add_col)
            rename_mapping[add_col] = new_name

    # Select and rename decoder columns
    decoder_subset = decoder_df.select(decoder_select_cols)
    if rename_mapping:
        decoder_subset = decoder_subset.rename(rename_mapping)

    # Fix datatype mismatch by casting join columns to compatible types
    try:
        main_dtype = main_df[main_column_converted].dtype
        decoder_dtype = decoder_subset[join_column_converted].dtype

        if main_dtype != decoder_dtype:
            logger.print(f"[yellow]âš ï¸  Datatype mismatch detected: main={main_dtype}, decoder={decoder_dtype}")

            # Try casting decoder to main type first
            try:
                logger.print(f"[cyan]ğŸ”§ Attempting to cast decoder column {join_column_converted} to {main_dtype}")
                decoder_subset = decoder_subset.with_columns(
                    pl.col(join_column_converted).cast(main_dtype)
                )
                logger.print(f"[green]âœ… Successfully cast decoder column to {main_dtype}")
            except Exception as cast_error:
                # If that fails, try casting both to string as fallback
                logger.print(f"[yellow]âš ï¸  Decoderâ†’main casting failed: {str(cast_error)}")
                logger.print(f"[cyan]ğŸ”§ Falling back to String casting for both columns")

                decoder_subset = decoder_subset.with_columns(
                    pl.col(join_column_converted).cast(pl.String)
                )
                # We'll perform join with main column cast to string on-the-fly
                main_column_for_join = main_column_converted
                logger.print(f"[green]âœ… Both columns will be joined as String type")

    except Exception as e:
        logger.print(f"[yellow]âš ï¸  Could not fix datatype mismatch: {str(e)}")

    # Perform left join (with string fallback if needed)
    try:
        result_df = main_df.join(
            decoder_subset,
            left_on=main_column_converted,
            right_on=join_column_converted,
            how="left"
        )
    except Exception as join_error:
        # Last resort: cast main column to string on-the-fly for join
        logger.print(f"[yellow]âš ï¸  Join failed, trying string fallback: {str(join_error)}")
        try:
            result_df = main_df.with_columns(
                pl.col(main_column_converted).cast(pl.String).alias(f"{main_column_converted}_str_temp")
            ).join(
                decoder_subset,
                left_on=f"{main_column_converted}_str_temp",
                right_on=join_column_converted,
                how="left"
            ).drop(f"{main_column_converted}_str_temp")
            logger.print(f"[green]âœ… String fallback join successful")
        except Exception as fallback_error:
            logger.print(f"[red]âŒ Both join attempts failed: {str(fallback_error)}")
            return main_df

    logger.print(f"[green]  âœ… Joined {len(additional_columns)} columns from {mapping_config.get('decoder_file', 'decoder')}")
    return result_df

def perform_complex_join(main_df, decoder_df, mapping_config, case_style="snake_case", logger=None):
    """
    Perform a complex multi-column join operation
    """
    if logger is None:
        logger = console

    main_column = mapping_config['main_column']
    join_columns = mapping_config['join_columns']
    additional_columns = mapping_config['additional_columns']


    # Apply case conversion
    main_column_converted = convert_case(main_column, case_style)
    join_columns_converted = [convert_case(col, case_style) for col in join_columns]
    additional_columns_converted = [convert_case(col, case_style) for col in additional_columns]

    # For complex joins, we need to map main columns to decoder columns
    # For vestiging example: we join on [Instelling, Vestigingsnummer] -> [Brinnummer, Vestigingsnummer]

    # Map join columns to their corresponding main data columns
    main_join_columns = []
    for i, join_col in enumerate(join_columns):
        if join_col == "Brinnummer":
            # Map to "Instelling van de hoogste vooropleiding"
            main_join_columns.append(convert_case("Instelling van de hoogste vooropleiding", case_style))
        elif join_col == "Vestigingsnummer":
            # Use the main column itself
            main_join_columns.append(main_column_converted)
        else:
            # Default: use the join column name
            main_join_columns.append(convert_case(join_col, case_style))

    # Check if all main columns exist
    missing_main_cols = [col for col in main_join_columns if col not in main_df.columns]
    if missing_main_cols:
        logger.print(f"[yellow]âš ï¸  Main columns not found: {missing_main_cols}")
        return main_df

    # Check if join columns exist in decoder
    missing_decoder_cols = [col for col in join_columns_converted if col not in decoder_df.columns]
    if missing_decoder_cols:
        logger.print(f"[yellow]âš ï¸  Decoder columns not found: {missing_decoder_cols}")
        return main_df

    # Check uniqueness of join columns in decoder
    is_unique, msg = check_unique_join_columns(decoder_df, join_columns_converted)
    if not is_unique:
        logger.print(f"[red]âŒ {msg}")
        return main_df

    # Prepare decoder columns with prefixes
    decoder_select_cols = join_columns_converted.copy()
    rename_mapping = {}

    for add_col in additional_columns_converted:
        if add_col in decoder_df.columns:
            # Generate prefix using MAIN COLUMN to ensure uniqueness
            prefix = generate_column_prefix(main_column_converted, add_col.replace('_', ' '), case_style)
            new_name = f"{prefix}"
            decoder_select_cols.append(add_col)
            rename_mapping[add_col] = new_name

    # Select and rename decoder columns
    decoder_subset = decoder_df.select(decoder_select_cols)
    if rename_mapping:
        decoder_subset = decoder_subset.rename(rename_mapping)

    # Fix datatype mismatches by casting decoder join columns to match main data types
    try:
        need_casting = False
        cast_operations = []

        for main_col, decoder_col in zip(main_join_columns, join_columns_converted):
            main_dtype = main_df[main_col].dtype
            decoder_dtype = decoder_subset[decoder_col].dtype

            if main_dtype != decoder_dtype:
                logger.print(f"[yellow]âš ï¸  Datatype mismatch detected: {decoder_col} main={main_dtype}, decoder={decoder_dtype}")
                logger.print(f"[cyan]ğŸ”§ Casting decoder column {decoder_col} to {main_dtype}")
                cast_operations.append(pl.col(decoder_col).cast(main_dtype))
                need_casting = True

        if need_casting:
            decoder_subset = decoder_subset.with_columns(cast_operations)
    except Exception as e:
        logger.print(f"[yellow]âš ï¸  Could not fix datatype mismatches: {str(e)}")

    # Perform left join on multiple columns
    result_df = main_df.join(
        decoder_subset,
        left_on=main_join_columns,
        right_on=join_columns_converted,
        how="left"
    )

    logger.print(f"[green]  âœ… Complex join: {len(additional_columns)} columns from {mapping_config.get('decoder_file', 'decoder')}")
    return result_df

def combine_all_data(config_path: str | None = None,
                    mapping_tables_config_path: str | None = None,
                    manual_tables_config_path: str | None = None,
                    processed_dir: str | None = None,
                    input_dir: str | None = None,
                    output_dir: str | None = None,
                    case_style: str = "snake_case"):
    """
    Main function to combine all data according to YAML configuration.

    Uses the storage abstraction layer for all file I/O operations.
    Defaults are loaded from environment variables via config.paths.
    """
    # Get configuration for defaults
    config = get_config()

    # Apply defaults from environment configuration
    config_path = config_path or "decoding_files_config.yaml"
    mapping_tables_config_path = mapping_tables_config_path or "mapping_tables_config.yaml"
    manual_tables_config_path = manual_tables_config_path or "manual_tables_config.yaml"
    processed_dir = processed_dir or config.paths.processed_dir
    input_dir = input_dir or config.paths.processed_dir
    output_dir = output_dir or config.paths.combined_dir

    # Initialize logger
    logger = CombinerLogger(output_dir)

    logger.print("[bold blue]ğŸ”„ Starting data combination process...[/bold blue]")
    logger.print(f"[cyan]Settings: case_style={case_style}")

    # Load decoder configuration
    config = load_yaml_config(config_path, logger)
    mappings = config['decoder_mappings']
    settings = config.get('settings', {})

    # Load main data
    main_data = load_main_data(processed_dir, logger)
    if not main_data:
        logger.print("[red]âŒ No main data found")
        return

    # Load mapping tables configuration and reference data
    logger.print(f"\n[bold magenta]ğŸ“š Loading reference data configuration...[/bold magenta]")
    mapping_config, active_files = load_mapping_tables_config(mapping_tables_config_path, logger)

    # Determine reference directory
    reference_dir = "data/reference"
    if mapping_config and 'settings' in mapping_config and 'reference_dir' in mapping_config['settings']:
        reference_dir = mapping_config['settings']['reference_dir']

    # Load reference data with active file filtering
    reference_data, missing_files, inactive_files = load_reference_data(reference_dir, active_files, logger)

    # Load manual mapping tables configuration and data
    logger.print(f"\n[bold yellow]ğŸ¯ Loading manual mapping configuration...[/bold yellow]")
    manual_config, manual_active_mappings = load_manual_mapping_config(manual_tables_config_path, logger)

    # Determine manual mapping directory
    manual_dir = "data/reference/manual"
    if manual_config and 'settings' in manual_config and 'manual_dir' in manual_config['settings']:
        manual_dir = manual_config['settings']['manual_dir']

    # Load manual mapping data with configurations
    manual_data, mapping_configs, manual_missing_files = load_manual_mapping_data(manual_dir, manual_active_mappings, logger)

    # Use storage abstraction for output operations
    with storage_context() as storage:
        # Create output directory
        storage.makedirs(output_dir)

        # Global tracking for reference joins
        all_non_working_files = []
        total_reference_joins = 0
        total_reference_successes = 0
        total_reference_failures = 0

        # Global tracking for manual mappings
        total_manual_mappings = 0
        total_manual_successes = 0
        total_manual_failures = 0

        # Process each main data file
        for data_name, main_df in main_data.items():
            logger.print(f"\n[bold cyan]ğŸ“Š Processing: {data_name}[/bold cyan]")

            combined_df = main_df.clone()
            successful_joins = 0
            failed_joins = 0

            # Process each mapping
            with Progress(
                SpinnerColumn(),
                TextColumn("[cyan]Applying joins..."),
                BarColumn(),
                TextColumn("[progress.percentage]{task.percentage:>3.0f}%"),
                TimeElapsedColumn(),
                console=console,
            ) as progress:

                task = progress.add_task("", total=len(mappings))

                for mapping_name, mapping_config in mappings.items():
                    decoder_file = mapping_config.get('decoder_file')
                    decoder_files = mapping_config.get('decoder_files')

                    # Handle both single decoder_file and multiple decoder_files
                    files_to_process = []
                    if decoder_file:
                        files_to_process.append(decoder_file)
                    elif decoder_files:
                        # For simplified version, just take the first file
                        files_to_process.append(decoder_files[0]['file'])

                    for dec_file in files_to_process:
                        # Load decoder data
                        decoder_df = load_decoder_data(dec_file, input_dir, logger)
                        if decoder_df is None:
                            failed_joins += 1
                            continue

                        try:
                            # Check if it's a complex join
                            if 'join_columns' in mapping_config:
                                combined_df = perform_complex_join(combined_df, decoder_df, mapping_config, case_style, logger)
                            else:
                                combined_df = perform_simple_join(combined_df, decoder_df, mapping_config, case_style, logger)

                            successful_joins += 1
                        except Exception as e:
                            logger.print(f"[red]âŒ Join failed for {mapping_name}: {str(e)}")
                            failed_joins += 1

                    progress.update(task, advance=1)

            # Add reference data joins
            logger.print(f"\n[bold magenta]ğŸ“š Adding reference labels for {data_name}...[/bold magenta]")
            combined_df, ref_successes, ref_failures, non_working = perform_reference_joins(
                combined_df, reference_data, case_style, logger
            )

            # Update global tracking
            total_reference_joins += (ref_successes + ref_failures)
            total_reference_successes += ref_successes
            total_reference_failures += ref_failures
            all_non_working_files.extend(non_working)

            # Apply manual categorical mappings
            logger.print(f"\n[bold yellow]ğŸ¯ Applying manual categorical mappings for {data_name}...[/bold yellow]")
            combined_df, manual_successes, manual_failures = apply_manual_mappings(
                combined_df, manual_data, mapping_configs, logger
            )

            # Update global manual mapping tracking
            total_manual_mappings += (manual_successes + manual_failures)
            total_manual_successes += manual_successes
            total_manual_failures += manual_failures

            # Save combined data using storage abstraction
            output_file = f"{output_dir}/{data_name}_combined.csv"
            storage.write_dataframe(combined_df, output_file, format="csv")

            # Summary
            logger.print(f"\n[bold green]âœ… Completed: {data_name}[/bold green]")
            logger.print(f"  ğŸ“Š Original columns: {len(main_df.columns)}")
            logger.print(f"  ğŸ“Š Final columns: {len(combined_df.columns)}")
            logger.print(f"  ğŸ“Š Added columns: {len(combined_df.columns) - len(main_df.columns)}")
            logger.print(f"  âœ… Successful decoder joins: {successful_joins}")
            logger.print(f"  âŒ Failed decoder joins: {failed_joins}")
            logger.print(f"  âœ… Successful reference joins: {ref_successes}")
            logger.print(f"  âŒ Failed reference joins: {ref_failures}")
            logger.print(f"  ğŸ¯ Successful manual mappings: {manual_successes}")
            logger.print(f"  âŒ Failed manual mappings: {manual_failures}")
            logger.print(f"  ğŸ’¾ Saved to: {output_file}")

    # Final summary for reference data
    logger.print(f"\n[bold magenta]ğŸ“š Reference Data Summary[/bold magenta]")
    logger.print(f"  ğŸ“Š Total reference files loaded: {len(reference_data)}")
    logger.print(f"  âœ… Total successful reference joins: {total_reference_successes}")
    logger.print(f"  âŒ Total failed reference joins: {total_reference_failures}")

    # Report configuration status
    if active_files is not None:
        logger.print(f"  ğŸ“‹ Files configured as active: {len(active_files)}")
        if missing_files:
            logger.print(f"  âš ï¸  Missing configured files: {missing_files}")
        if inactive_files:
            logger.print(f"  ğŸ’¤ Files not in config (inactive): {len(inactive_files)} files")

    # Final summary for manual mappings
    logger.print(f"\n[bold yellow]ğŸ¯ Manual Mapping Summary[/bold yellow]")
    logger.print(f"  ğŸ“Š Total manual mapping files loaded: {len(manual_data)}")
    logger.print(f"  âœ… Total successful manual mappings: {total_manual_successes}")
    logger.print(f"  âŒ Total failed manual mappings: {total_manual_failures}")

    # Report manual mapping configuration status
    if manual_active_mappings is not None:
        logger.print(f"  ğŸ“‹ Manual mappings configured as active: {len(manual_active_mappings)}")
        if manual_missing_files:
            logger.print(f"  âš ï¸  Missing manual configured files: {manual_missing_files}")

    # Report non-working files
    if all_non_working_files:
        logger.print(f"\n[bold red]âŒ Non-working reference files ({len(all_non_working_files)}):[/bold red]")
        for nw in all_non_working_files:
            logger.print(f"  [red]â€¢ {nw['file']} (column: {nw['column']}): {nw['error']}")
    else:
        logger.print(f"[bold green]ğŸ‰ All reference files processed successfully![/bold green]")

    # Save logs to files
    logger.save_logs()

    # Return log info with reference summary
    log_info = logger.get_log_files()
    log_info['reference_summary'] = {
        'total_files': len(reference_data),
        'successful_joins': total_reference_successes,
        'failed_joins': total_reference_failures,
        'non_working_files': all_non_working_files,
        'missing_configured_files': missing_files,
        'inactive_files': inactive_files,
        'active_files_count': len(active_files) if active_files else 0
    }
    return log_info

def main():
    """
    Command line entry point.

    Uses environment variable defaults from config.paths when no arguments provided.
    """
    # Get config for environment-based defaults
    config = get_config()

    parser = argparse.ArgumentParser(description='Combine main data with decoder files')
    parser.add_argument('--config', default='decoding_files_config.yaml',
                       help='Path to YAML decoder configuration file')
    parser.add_argument('--mapping-tables-config', default='mapping_tables_config.yaml',
                       help='Path to YAML mapping tables configuration file')
    parser.add_argument('--manual-tables-config', default='manual_tables_config.yaml',
                       help='Path to YAML manual mapping tables configuration file')
    parser.add_argument('--processed-dir', default=None,
                       help=f'Directory with processed main data (default: {config.paths.processed_dir})')
    parser.add_argument('--input-dir', default=None,
                       help=f'Directory with decoder files (default: {config.paths.processed_dir})')
    parser.add_argument('--output-dir', default=None,
                       help=f'Output directory for combined data (default: {config.paths.combined_dir})')
    parser.add_argument('--case-style', default='snake_case',
                       choices=['original', 'snake_case', 'camelCase', 'PascalCase'],
                       help='Case style used in processed data')

    args = parser.parse_args()

    log_info = combine_all_data(
        config_path=args.config,
        mapping_tables_config_path=args.mapping_tables_config,
        manual_tables_config_path=args.manual_tables_config,
        processed_dir=args.processed_dir,
        input_dir=args.input_dir,
        output_dir=args.output_dir,
        case_style=args.case_style
    )

    if log_info:
        print(f"Logs saved: {log_info['timestamped_log']}")
        return log_info

if __name__ == "__main__":
    main()
